{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zWDlw25eEjA",
        "outputId": "a18585a3-958f-4b7c-aa9d-40847f887de3"
      },
      "id": "5zWDlw25eEjA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50699f86-bf4f-4601-9602-55dbd1756552",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50699f86-bf4f-4601-9602-55dbd1756552",
        "outputId": "be0c762c-a917-4867-afdc-4b385e94cc78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "import os\n",
        "import logging\n",
        "import math\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b51fd2fa-8712-4917-9e6a-171d3a0ec88c",
      "metadata": {
        "id": "b51fd2fa-8712-4917-9e6a-171d3a0ec88c"
      },
      "outputs": [],
      "source": [
        "def read_documents(path):\n",
        "\n",
        "    content = {}\n",
        "    doc_id_filename = {}\n",
        "    doc_id = 0\n",
        "    for filename in os.listdir(path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(os.path.join(path, filename), 'r', encoding='utf-8') as file:\n",
        "                content[doc_id] = file.read()\n",
        "                doc_id_filename[doc_id] = filename\n",
        "                logging.info(f\"Loaded file: {filename} with doc_id: {doc_id}\")\n",
        "                doc_id += 1\n",
        "    return content, doc_id_filename"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " Tokenization"
      ],
      "metadata": {
        "id": "t5u3iHQTIB_x"
      },
      "id": "t5u3iHQTIB_x"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08a0b276-3f21-42bd-9571-417fb2b8e065",
      "metadata": {
        "id": "08a0b276-3f21-42bd-9571-417fb2b8e065"
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "\n",
        "    if isinstance(text, str):\n",
        "        return text.lower().split()\n",
        "    else:\n",
        "        return []"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning"
      ],
      "metadata": {
        "id": "B0rOnO8HIHMp"
      },
      "id": "B0rOnO8HIHMp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd853c6e-5ec6-4f07-9000-682c669c80fd",
      "metadata": {
        "id": "fd853c6e-5ec6-4f07-9000-682c669c80fd"
      },
      "outputs": [],
      "source": [
        "def text_cleaning(text):\n",
        "\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    cleaned_text = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "    return cleaned_text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frequency Calculation"
      ],
      "metadata": {
        "id": "aGrwLpNZIe2f"
      },
      "id": "aGrwLpNZIe2f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12150862-6a06-4a80-b55d-da6863fde524",
      "metadata": {
        "id": "12150862-6a06-4a80-b55d-da6863fde524"
      },
      "outputs": [],
      "source": [
        "def term_frequency(term, document):\n",
        "    return document.count(term) / len(document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e2e5b0a-0009-4a73-ab28-bdfb0b04916d",
      "metadata": {
        "id": "2e2e5b0a-0009-4a73-ab28-bdfb0b04916d"
      },
      "outputs": [],
      "source": [
        "def inverse_document_frequency(term, all_documents):\n",
        "    num_docs_containing_term = sum(1 for doc in all_documents if term in doc)\n",
        "    return math.log(len(all_documents) / (1 + num_docs_containing_term))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9504ba75-84d9-42af-a535-6ceb00abe53d",
      "metadata": {
        "id": "9504ba75-84d9-42af-a535-6ceb00abe53d"
      },
      "outputs": [],
      "source": [
        "def compute_tfidf(document, all_documents, vocab):\n",
        "    tfidf_vector = []\n",
        "    for term in vocab:\n",
        "        tf = term_frequency(term, document)\n",
        "        idf = inverse_document_frequency(term, all_documents)\n",
        "        tfidf_vector.append(tf * idf)\n",
        "    return np.array(tfidf_vector)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    return dot_product / (norm_vec1 * norm_vec2 + 1e-8)"
      ],
      "metadata": {
        "id": "1DdcfI_trLIV"
      },
      "id": "1DdcfI_trLIV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ee35d07-7a5c-4b1e-b4df-a8871d8732a2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ee35d07-7a5c-4b1e-b4df-a8871d8732a2",
        "outputId": "0aeab60c-d354-4903-e038-17965deb8ecf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "[(' electric', [(0, 0.065458442965528), (4, 0.037985988096978916), (3, 0.037773195424594105), (5, 0.030146567959790007), (6, 0.028514332334712542), (1, 0.0), (2, 0.0), (7, 0.0), (8, 0.0), (9, 0.0)])]\n",
            "[(' electric', [(0, 0.065458442965528), (4, 0.037985988096978916), (3, 0.037773195424594105), (5, 0.030146567959790007), (6, 0.028514332334712542), (1, 0.0), (2, 0.0), (7, 0.0), (8, 0.0), (9, 0.0)]), (' sedan', [(8, 0.11012353979271548), (5, 0.10815050649759479), (6, 0.10229487698065012), (0, 0.0), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (7, 0.0), (9, 0.0)])]\n",
            "[(' electric', [(0, 0.065458442965528), (4, 0.037985988096978916), (3, 0.037773195424594105), (5, 0.030146567959790007), (6, 0.028514332334712542), (1, 0.0), (2, 0.0), (7, 0.0), (8, 0.0), (9, 0.0)]), (' sedan', [(8, 0.11012353979271548), (5, 0.10815050649759479), (6, 0.10229487698065012), (0, 0.0), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (7, 0.0), (9, 0.0)]), (' car', [(9, 0.09003668375064869), (4, 0.07597197619395783), (7, 0.04297548635336403), (3, 0.037773195424594105), (2, 0.02409712543385022), (0, 0.0), (1, 0.0), (5, 0.0), (6, 0.0), (8, 0.0)])]\n",
            "[(' electric', [(0, 0.065458442965528), (4, 0.037985988096978916), (3, 0.037773195424594105), (5, 0.030146567959790007), (6, 0.028514332334712542), (1, 0.0), (2, 0.0), (7, 0.0), (8, 0.0), (9, 0.0)]), (' sedan', [(8, 0.11012353979271548), (5, 0.10815050649759479), (6, 0.10229487698065012), (0, 0.0), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (7, 0.0), (9, 0.0)]), (' car', [(9, 0.09003668375064869), (4, 0.07597197619395783), (7, 0.04297548635336403), (3, 0.037773195424594105), (2, 0.02409712543385022), (0, 0.0), (1, 0.0), (5, 0.0), (6, 0.0), (8, 0.0)]), ('family car', [(9, 0.09003665164851364), (4, 0.07597194345815268), (7, 0.042975474816607157), (3, 0.03777318246562961), (2, 0.024097118668993688), (0, 0.0), (1, 0.0), (5, 0.0), (6, 0.0), (8, 0.0)])]\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "  path = \"/content/drive/MyDrive/Tech_cars\"\n",
        "  docs, doc_id_to_filename = read_documents(path)\n",
        "\n",
        "  queries = [ \" electric\", \" sedan\", \" car\", \"family car\"]\n",
        "\n",
        "  tokenized_docs = [tokenize(doc) for doc in docs.values()]\n",
        "\n",
        "  vocab = sorted(set(word for doc in tokenized_docs for word in doc))\n",
        "  print(logging.info(f\"Vocabulary size: {len(vocab)}\"))\n",
        "\n",
        "  doc_tfidf_vectors = [compute_tfidf(doc, tokenized_docs, vocab) for doc in tokenized_docs]\n",
        "\n",
        "  results = []\n",
        "\n",
        "  for query in queries:\n",
        "        tokenized_query = tokenize(query)\n",
        "        query_tfidf_vector = compute_tfidf(tokenized_query, tokenized_docs, vocab)\n",
        "\n",
        "        similarities = []\n",
        "        for doc_id, doc_vector in enumerate(doc_tfidf_vectors):\n",
        "            similarity = cosine_similarity(query_tfidf_vector, doc_vector)\n",
        "            similarities.append((doc_id, similarity))\n",
        "\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        results.append((query, similarities))\n",
        "\n",
        "        print(results)\n",
        "\n",
        "  result_path = \"/content/drive/MyDrive/Tech400/results\"\n",
        "  output_file = os.path.join(result_path, \"results_resha_maharjan.txt\")\n",
        "  with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for query, similarities in results:\n",
        "          f.write(f\"Query: {query}\\n\")\n",
        "          for doc_id, similarity in similarities:\n",
        "              filename = doc_id_to_filename[doc_id]\n",
        "              f.write(f\"  Document: {filename}, Similarity: {similarity:.4f}\\n\")\n",
        "          f.write(\"\\n\")\n",
        "\n",
        "          logging.info(f\"Results written to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I7KtVMscrmi9"
      },
      "id": "I7KtVMscrmi9",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}